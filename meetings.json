{
  "nextMeeting": {
    "date": "12.02.2024",
    "time": "17:00",
    "room": "ascii",
    "papers": [
            {
        "title": "Attention Is All You Need",
        "link": "https://arxiv.org/pdf/1706.03762"
      },
      {
        "title": "Improving Language Understanding by Generative Pre-Training",
        "link": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
      }
    ]
  },
  "pastMeetings": [
    {
    "date": "30.01.2024",
    "time": "18:00",
    "room": "??? (wird noch bekannt gegeben)",
    "papers": [
      {
        "title": "Improving Language Understanding by Generative Pre-Training",
        "link": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
      }
    ],
      "slides": "slides2025/02_gpt1.pdf"
  },
    {
    "date": "17.01.2024",
    "time": "17:00",
    "room": "E005",
    "papers": [
      {
        "title": "Attention Is All You Need",
        "link": "https://arxiv.org/pdf/1706.03762"
      }
    ],
      "slides": "slides2025/01_attention_is_all_you_need.pdf"
  },
    {
    "date": "04.03.2024",
    "time": "17:00",
    "room": "3105?",
    "papers": [
      {
        "title": "Locating and Editing Factual Associations in GPT",
        "link": "https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf"
      }
    ],
    "slides": "slides/16-Rome.pdf"
  },
    {
    "date": "19.02.2024",
    "time": "17:00",
    "room": "3105",
    "papers": [
      {
        "title": "Linformer: Self-Attention with Linear Complexity",
        "link": "https://arxiv.org/pdf/2006.04768.pdf"
      }
    ],
      "slides": "slides/15-Linformer.pdf"
  },
    {
    "date": "31.01.2024",
    "time": "17:00",
    "room": "3045",
    "papers": [
      {
        "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "link": "https://arxiv.org/pdf/2305.07759.pdf"
      }
    ],
      "slides": "slides/14-tinystories.pdf"
  },
    {
    "date": "08.01.2024",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
        "link": "https://arxiv.org/pdf/2205.14135.pdf"
      }
    ],
      "slides": "slides/13-flashattention.pdf"
  },
    {"date": "18.12.2023",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ",
        "link": "https://dl.acm.org/doi/abs/10.1145/3442188.3445922"
      }
    ],
      "slides":"slides/12-StochasticParrot.pdf"
    },
    {
    "date": "27.11. und 4.12.2023",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "A Mathematical Framework for Transformer Circuits",
        "link": "https://transformer-circuits.pub/2021/framework/index.html"
      },
      {
        "title": "Video Playlist der Autoren (hat mehr Inhalt als im Paper steht)",
        "link": "https://www.youtube.com/watch?v=V3NQaDR3xI4&list=PLoyGOS2WIonajhAVqKUgEMNmeq3nEeM51&index=2"
      }
    ],
      "slides": "slides/10-11-transformer-circuits.pdf" 
  },
    
    {
    "date": "20.11.2023",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "link": "https://arxiv.org/pdf/2106.09685.pdf"
      }
    ],
      "slides": "slides/09-lora.pdf"
  },
    {
    "date": "06.11.2023",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
        "link": "https://arxiv.org/pdf/1803.03635.pdf"
      }
    ],
      "slides": "slides/08-lottery-ticket.pdf"
  },
    {
    "date": "30.10.2023",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "Scaling Laws for Neural Language Models",
        "link": "https://arxiv.org/pdf/2001.08361.pdf"
      }
    ],
      "slides": "slides/07-Scaling Laws for Neural Language Models.pdf"
  },
    {
    "date": "16.10.2023",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "link": "https://arxiv.org/pdf/1810.04805.pdf"
      }
    ],
      "slides": "slides/06-bert.pdf"
  },
    {
    "date": "09.10.2023",
    "time": "17:00",
    "room": "E001",
    "papers": [
      {
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "link": "https://arxiv.org/pdf/2302.13971.pdf"
      }
    ],
      "slides": "slides/05-llama-open-and-efficient-foundation-models.pdf"
    },
    {
    "date": "11.09.2023",
    "time": "17:00",
    "room": "?",
    "papers": [
      {
        "title": "Training language models to follow instructions with human feedback",
        "link": "https://arxiv.org/pdf/2203.02155.pdf"
      }
    ],
      "slides": "slides/04-instructgpt.pdf"
  },
    {
    "date": "04.09.2023",
    "time": "17:00",
    "room": "3045",
    "papers": [
      {
        "title": "Language Models are Unsupervised Multitask Learners",
        "link": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
      },
      {
        "title": "Language Models are Few-Shot Learners",
        "link": "https://arxiv.org/pdf/2005.14165.pdf"
      }
    ],
      "slides": "slides/03-gpt_2_and_3.pdf"
  },
    {
      "date": "28.08.2023",
      "time": "17:00",
      "room": "E001",
      "papers": [
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "link": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
        }
      ],
      "slides": "slides/02-Improving Language Understanding by Generative Pre-Training.pdf"
    },
    {
      "date": "21.08.2023",
      "time": "17:00",
      "room": "E001",
      "papers": [
        {
          "title": "Attention Is All You Need",
          "link": "https://arxiv.org/pdf/1706.03762.pdf"
        }
      ],
      "slides": "slides/01-attention_is_all_you_need.pdf"
    }
  ]
}
